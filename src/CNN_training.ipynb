{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from CNN_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defino las constantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "img_path = '../input/biopsy_images/'\n",
    "dataset_path = \"../output/df_imagenes_completo.csv\"\n",
    "\n",
    "#img size\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "number_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of images\n",
    "input_shape = (img_rows, img_cols, number_channels)\n",
    "# Number of labels\n",
    "num_classes = 2\n",
    "\n",
    "#NN topology\n",
    "model = Sequential()\n",
    "\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    chanDim = 1\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    117273\n",
       "0     75039\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].replace(\"MSI\",0)\n",
    "df[\"label\"] = df[\"label\"].replace(\"MSS\",1)\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aprox. 400 imagenes para validación\n",
    "train_ratio=0.002\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.image, df.label, test_size=train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model at the end of every epoch if val_accuracy is better \n",
    "filepath='Checkpoint_{epoch:02d}_{val_accuracy:.2f}'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flori/.local/lib/python3.7/site-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "batchs = 40\n",
    "epochs = 60\n",
    "img_number=192312\n",
    "\n",
    "#Fit model\n",
    "trainGen=generator(X_train, y_train, batchs*(1-train_ratio), img_path)\n",
    "testGen=generator(X_test, y_test, batchs*train_ratio, img_path)\n",
    "\n",
    "model.fit_generator(\n",
    "    trainGen,\n",
    "    steps_per_epoch=img_number*(1-train_ratio)//batchs,\n",
    "    validation_data=testGen,\n",
    "    validation_steps=img_number*train_ratio//batchs,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.history['accuracy']\n",
    "val_acc = model.history['val_accuracy']\n",
    "loss = model.history['loss']\n",
    "val_loss = model.history['val_loss']\n",
    "\n",
    "plot_acc_loss(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGen = generator(X_test, y_test, batchs, img_path, mode=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(testGen, steps=img_number*train_ratio//batchs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIdxs = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_score)\n",
    "roc = roc_curve(y_test.values, predIdxs)\n",
    "display(pd.DataFrame({\n",
    "    \"gt\":y_test.values,\n",
    "    \"predicted\":predIdxs,\n",
    "    \"proba MSI\":predictions[:,0],\n",
    "    \"proba MSS\":predictions[:,1]\n",
    "}).head())\n",
    "\n",
    "plt.plot(roc[0],roc[1])\n",
    "plt.legend([\"AUC: {}\".format(round(roc_auc_score(y_test.values, predictions[:,1]),2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test.values, predIdxs)\n",
    "\n",
    "# Visualiamos la matriz de confusión\n",
    "cm_df = pd.DataFrame(cm)  \n",
    "plt.figure(figsize = (7,5))  \n",
    "sn.set(font_scale=1) #for label size  \n",
    "sn.heatmap(cm_df, annot=True, annot_kws={\"size\": 12}, fmt=\"d\") # font size  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_CNN.json\", \"w\") as json_file:\n",
    "    json.dump(model_json, json_file)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_prueba.json','r') as f:\n",
    "    model_json = json.load(f)\n",
    "loaded_model = model_from_json(model_json)\n",
    "loaded_model.load_weights(#poner el path del checkpoint bueno\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
